{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/zeeshanahmad10809/covid_fake_news_classification/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Anaysis (Deep Learning Techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPd1FfPhrU2B",
    "outputId": "6f365e7c-03ab-49e6-f68f-810c2b4bc62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in c:\\users\\home\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from loguru) (1.1.0)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\home\\anaconda3\\lib\\site-packages (from loguru) (0.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\home\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\home\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBViJbJ-W0_y",
    "outputId": "48a21a68-5d69-46b9-d0ac-c4ba94872d2e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing import text, sequence\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, BatchNormalization, Dense, Dropout\n",
    "# from tensorflow.keras.layers import (\n",
    "#     GRU,\n",
    "#     Bidirectional,\n",
    "#     SimpleRNN,\n",
    "#     Conv1D,\n",
    "#     GlobalMaxPool1D,\n",
    "# )\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_BKj9TGVLDs",
    "outputId": "0f23db5b-8870-42d7-8c8f-b8b50bee833f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !wget = download (web get)\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
    "!unzip crawl-300d-2M.vec.zip # 解壓縮\n",
    "!rm crawl-300d-2M.vec.zip #(remove) delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CFQGz6nQD6xc"
   },
   "outputs": [],
   "source": [
    "# Set random seed values to attain deterministic behaviours\n",
    "SEED_VALUE = 19 #讓他沒有隨機性\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "tf.random.set_seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cV8_0e-ucLu8"
   },
   "outputs": [],
   "source": [
    "def remove_url(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    for ch in string.punctuation:\n",
    "        if ch in tweet:\n",
    "            tweet = tweet.replace(ch, \"\")\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def lower_case(tweet):\n",
    "    return tweet.lower().strip()\n",
    "\n",
    "\n",
    "def lemmatize(tweet):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweet = \" \".join(lemmatizer.lemmatize(token) for token in tweet.split(\" \"))\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # tweet = p.clean(tweet)\n",
    "    tweet = remove_url(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = lower_case(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dgxuJ7w0yN7l"
   },
   "outputs": [],
   "source": [
    "DATASET1 = \"COVID FakeNews Data.csv\"\n",
    "DATASET2 = \"dataset-Non-extremist-Extremist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oizUVH7HrcN0"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        data = None\n",
    "        try:\n",
    "            data = pd.read_csv(self.dataset_name)\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"Dataset File is missing!\")\n",
    "            os._exit(0)\n",
    "        if self.dataset_name == \"dataset-Non-extremist-Extremist.csv\":\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Non-extremist\", 0)\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Extremist\", 1)\n",
    "            col_list = data.columns.to_list()\n",
    "            col_list = [col_list[-1], col_list[0]]\n",
    "            data = data[col_list]\n",
    "\n",
    "        data.iloc[:, 0] = data.iloc[:, 0].apply(preprocess_tweet)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            data.iloc[:, 0], data.iloc[:, 1], stratify=data.iloc[:, 1], test_size=0.2\n",
    "        )\n",
    "        self.embedding_size = 300\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def get_embedding_size(self):\n",
    "        return self.embedding_size\n",
    "\n",
    "    def get_word_index(self):\n",
    "        return self.tokenizer.word_index\n",
    "\n",
    "    def create_vocab(self):\n",
    "        self.words_set = set()\n",
    "        self.max_sentence_len = 0\n",
    "        for sentence in self.X_train:\n",
    "            tokens = sentence.split(\" \")\n",
    "            if len(tokens) > self.max_sentence_len:\n",
    "                self.max_sentence_len = len(tokens)\n",
    "            for word in tokens:\n",
    "                self.words_set.add(word)\n",
    "\n",
    "    def get_vocab_info(self):\n",
    "        return len(self.words_set), self.max_sentence_len\n",
    "\n",
    "    def fit(self):\n",
    "        self.create_vocab()\n",
    "        vocab_size, max_sentence_len = self.get_vocab_info()\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        self.tokenizer.fit_on_texts(self.X_train)\n",
    "\n",
    "    def load(self):\n",
    "        self.fit()\n",
    "        self.X_train = sequence.pad_sequences(\n",
    "            self.tokenizer.texts_to_sequences(self.X_train),\n",
    "            maxlen=self.max_sentence_len,\n",
    "            padding=\"post\",\n",
    "        )\n",
    "        self.X_test = sequence.pad_sequences(\n",
    "            self.tokenizer.texts_to_sequences(self.X_test),\n",
    "            maxlen=self.max_sentence_len,\n",
    "            padding=\"post\",\n",
    "        )\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PCR7hjUszVI"
   },
   "outputs": [],
   "source": [
    "def loadFastTextModel(path=\"\"):\n",
    "    logger.info(\"Loading FastText Model!\")\n",
    "    embeddings_index = dict()\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            with tqdm(total=1999996, desc=\"loading FastText\") as pbar:\n",
    "                for line in f:\n",
    "                    values = line.strip().split(\" \")\n",
    "                    word = values[0]\n",
    "                    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "                    embeddings_index[word] = coefs\n",
    "                    pbar.update(1)\n",
    "\n",
    "        return embeddings_index\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Embedding file not in path!\")\n",
    "        os._exit(0)\n",
    "\n",
    "\n",
    "def buildEmbeddingMatrix(word_index, vocab_size, embedding_size, embeddings_index):\n",
    "    logger.info(\"Building Embedding Matrix!\")\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0pZhA89Uw-Kp"
   },
   "outputs": [],
   "source": [
    "def LSTM_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            Dropout(0.1),\n",
    "            LSTM(64),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\", name=\"relu_dense1\"),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def GRU_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    initializer = tf.keras.initializers.GlorotUniform()\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            GRU(\n",
    "                64,\n",
    "                return_sequences=True,\n",
    "                kernel_initializer=initializer,\n",
    "                recurrent_initializer=initializer,\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            GRU(64, kernel_initializer=initializer, recurrent_initializer=initializer),\n",
    "            Dropout(0.3),\n",
    "            Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer,\n",
    "                name=\"relu_dense1\",\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def RNN_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    initializer = tf.keras.initializers.GlorotUniform()\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            SimpleRNN(\n",
    "                64,\n",
    "                return_sequences=True,\n",
    "                kernel_initializer=initializer,\n",
    "                recurrent_initializer=initializer,\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            SimpleRNN(\n",
    "                64, kernel_initializer=initializer, recurrent_initializer=initializer\n",
    "            ),\n",
    "            Dropout(0.3),\n",
    "            Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer,\n",
    "                name=\"relu_dense1\",\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def Conv1d_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            Conv1D(128, 3, strides=1, padding=\"SAME\", activation=\"relu\"),\n",
    "            Dropout(0.1),\n",
    "            Conv1D(256, 3, strides=1, padding=\"SAME\", activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            GlobalMaxPool1D(),\n",
    "            Dense(64, activation=\"relu\", name=\"relu_dense1\"),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def BiLSTM_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            Dropout(0.1),\n",
    "            Bidirectional(LSTM(64)),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\", name=\"relu_dense1\"),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def buildModel(\n",
    "    name=\"lstm\",\n",
    "    embedding_matrix=None,\n",
    "    vocab_size=16222,\n",
    "    embedding_size=None,\n",
    "    max_sen_len=56,\n",
    "    num_classes=2,\n",
    "):\n",
    "    if name == \"lstm\":\n",
    "        return LSTM_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"gru\":\n",
    "        return GRU_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"rnn\":\n",
    "        return RNN_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"bilstm\":\n",
    "        return BiLSTM_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"conv1d\":\n",
    "        return Conv1d_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"Invalid model name {name}\")\n",
    "        os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T535XUQOwTef",
    "outputId": "ca1b5b28-d3ca-4bab-c642-8249c3f7e6b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 02:40:14.787 | INFO     | __main__:loadFastTextModel:2 - Loading FastText Model!\n"
     ]
    }
   ],
   "source": [
    "fasttext_embeddings_index = loadFastTextModel(\"crawl-300d-2M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39VcOPt2wcWl",
    "outputId": "2dec7fd8-60cc-4800-da71-a608be9b9f7e"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(DATASET2)\n",
    "X_train, X_test, y_train, y_test = dataset.load()\n",
    "vocab_size, max_sentence_len = dataset.get_vocab_info()\n",
    "word_index = dataset.get_word_index()\n",
    "embedding_size = dataset.get_embedding_size()\n",
    "embedding_matrix = buildEmbeddingMatrix(\n",
    "    word_index, vocab_size, embedding_size, fasttext_embeddings_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CwFj-UtO0iS7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buildModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c51ec3898f09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = buildModel(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lstm\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0membedding_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0membedding_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'buildModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = buildModel(\n",
    "    name=\"lstm\",\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    max_sen_len=max_sentence_len,\n",
    "    num_classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eCtHziPb5EBB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-92925a6a6986>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.compile(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02zVLx9f5Z6G",
    "outputId": "b0d1ceda-40ce-45b9-a0c7-9695d7a6b099"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8e933a6fa521>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor=\"val_loss\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UdozvXn7PrF"
   },
   "outputs": [],
   "source": [
    "def show_performance(y_test, y_test_pred):\n",
    "    pprint(confusion_matrix(y_test, y_test_pred))\n",
    "    print(classification_report(y_test, y_test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSXNGSnI5ima"
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.where(model.predict(X_test) < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOh6Pcn27JCl",
    "outputId": "54f9c96d-4663-4e9f-cc19-4e5862ed6cd5"
   },
   "outputs": [],
   "source": [
    "# USE 'macro avg'\n",
    "show_performance(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
